{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPUnnnhK6YR3N4l7dYSnLFs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/txvuong150764/Sentence_Semantic_phoBERT/blob/master/QS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "kQJHAIWyJePf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02325f05-3c8e-422b-957d-793198179c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "_HcgWT13PJY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Exj4tWon_hOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "ZMT0eMSV91s_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0d74e96-14e9-4600-94d7-bf7402423f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'VnCoreNLP' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vncorenlp in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vncorenlp/VnCoreNLP.git\n",
        "!pip install vncorenlp\n",
        "from vncorenlp import VnCoreNLP\n",
        "annotator = VnCoreNLP('VnCoreNLP/VnCoreNLP-1.1.1.jar', annotators='wseg', max_heap_size='-Xmx500m')\n",
        "\n",
        "def word_segment(text):\n",
        "    word_segmented_text = annotator.tokenize(text) \n",
        "    sentences = []\n",
        "    for sentence in word_segmented_text:\n",
        "        sentences.append(' '.join(sentence))\n",
        "    return ' '.join(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = word_segment(\"Ngày quốc khánh của việt nam là 2/9\")\n",
        "s"
      ],
      "metadata": {
        "id": "cWeK7gAHDIHA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f35b85e2-5e53-4372-c21f-2f62f844bd9d"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ngày quốc_khánh của việt_nam là 2/9'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 6\n",
        "max_length = 128"
      ],
      "metadata": {
        "id": "lwlSFIvCDeFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset_df = pd.read_csv('/content/train.csv')\n",
        "valid_df = pd.read_csv('/content/dev.csv')"
      ],
      "metadata": {
        "id": "e1L3NsMjDfUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(dataset_df)):\n",
        "    dataset_df['question 1'][i] = word_segment(dataset_df['question 1'][i])\n",
        "    dataset_df['question 2'][i] = word_segment(dataset_df['question 2'][i])"
      ],
      "metadata": {
        "id": "EW4ilLjWFi-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09db6b52-7227-4e87-d573-278a5a13972b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(dataset_df, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "8RONLNbucB9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "p0flr9Oafr2z",
        "outputId": "745b5c1a-752c-446d-bbe1-b77168f92f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               question 1  \\\n",
              "53261               lý thái tổ mất vào ngày_tháng năm nào   \n",
              "123871                           vì sao phải mặc quần_lót   \n",
              "28417   ngày truyền_thống của quân_lực việt_nam cộng_h...   \n",
              "33182                      phim titanic phát_hành năm nào   \n",
              "83411   bảy kỳ_quan thế_giới mới được lựa_chọn như_thế...   \n",
              "...                                                   ...   \n",
              "59060          cha_mẹ của luiz inácio lula da silva là ai   \n",
              "83077                  bóng_chày được chơi phổ_biến ở đâu   \n",
              "15603   korkishko bắt_đầu sự_nghiệp bóng_đá tại câu_lạ...   \n",
              "19151                   ai đứng đầu tây_sơn thất hổ_tướng   \n",
              "20181    albert einstein được trao giải nobel vào năm nào   \n",
              "\n",
              "                                               question 2  similarity  \n",
              "53261                       isaac newton mất vào ngày nào       False  \n",
              "123871                    vì sao lại mà phải mặc quần_lót        True  \n",
              "28417   ngày truyền_thống của quân_lực việt_nam cộng_h...       False  \n",
              "33182      bộ manga nào có lượt phát_hành kỷ_lục năm 2008       False  \n",
              "83411       điều_kiện lựa_chọn thẩm_phán của mỗi_một viện       False  \n",
              "...                                                   ...         ...  \n",
              "59060   luiz inácio lula da silva giành thắng_lợi tron...       False  \n",
              "83077                     bóng_rổ phổ_biến ở các nước nào       False  \n",
              "15603   xavi từ_giã sự_nghiệp bóng_đá quốc_tế vào năm nào       False  \n",
              "19151   hổ_tướng nào nhà tây_sơn có vợ là nữ_tướng trầ...       False  \n",
              "20181   ai là người được trao giải nobel vật_lý vào nă...       False  \n",
              "\n",
              "[37354 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a331731-b9c0-4b92-9ec7-528943822666\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question 1</th>\n",
              "      <th>question 2</th>\n",
              "      <th>similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>53261</th>\n",
              "      <td>lý thái tổ mất vào ngày_tháng năm nào</td>\n",
              "      <td>isaac newton mất vào ngày nào</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123871</th>\n",
              "      <td>vì sao phải mặc quần_lót</td>\n",
              "      <td>vì sao lại mà phải mặc quần_lót</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28417</th>\n",
              "      <td>ngày truyền_thống của quân_lực việt_nam cộng_h...</td>\n",
              "      <td>ngày truyền_thống của quân_lực việt_nam cộng_h...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33182</th>\n",
              "      <td>phim titanic phát_hành năm nào</td>\n",
              "      <td>bộ manga nào có lượt phát_hành kỷ_lục năm 2008</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83411</th>\n",
              "      <td>bảy kỳ_quan thế_giới mới được lựa_chọn như_thế...</td>\n",
              "      <td>điều_kiện lựa_chọn thẩm_phán của mỗi_một viện</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59060</th>\n",
              "      <td>cha_mẹ của luiz inácio lula da silva là ai</td>\n",
              "      <td>luiz inácio lula da silva giành thắng_lợi tron...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83077</th>\n",
              "      <td>bóng_chày được chơi phổ_biến ở đâu</td>\n",
              "      <td>bóng_rổ phổ_biến ở các nước nào</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15603</th>\n",
              "      <td>korkishko bắt_đầu sự_nghiệp bóng_đá tại câu_lạ...</td>\n",
              "      <td>xavi từ_giã sự_nghiệp bóng_đá quốc_tế vào năm nào</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19151</th>\n",
              "      <td>ai đứng đầu tây_sơn thất hổ_tướng</td>\n",
              "      <td>hổ_tướng nào nhà tây_sơn có vợ là nữ_tướng trầ...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20181</th>\n",
              "      <td>albert einstein được trao giải nobel vào năm nào</td>\n",
              "      <td>ai là người được trao giải nobel vật_lý vào nă...</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>37354 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a331731-b9c0-4b92-9ec7-528943822666')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a331731-b9c0-4b92-9ec7-528943822666 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a331731-b9c0-4b92-9ec7-528943822666');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(valid_df)):\n",
        "    valid_df['question 1'][i] = word_segment(valid_df['question 1'][i])\n",
        "    valid_df['question 2'][i] = word_segment(valid_df['question 2'][i])"
      ],
      "metadata": {
        "id": "mjkKpvClNfX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9599d15a-11db-4686-a81e-8a4bea9c1163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(valid_df['question 1'][0], valid_df['question 2'][0])"
      ],
      "metadata": {
        "id": "jQk_kgXqI5iZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2705aa3-798f-4846-ebe2-c76428ecb3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "văn_hoá việt_nam có từ bao_giờ tên gọi hai chữ việt_nam và quốc_hiệu việt_nam có từ bao_giờ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
      ],
      "metadata": {
        "id": "L22FRYZBI8Dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf6a833-c577-4ce4-d217-b2dfca27be2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EquavilenceDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.df = df\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        To customize dataset, inherit from Dataset class and implement\n",
        "        __len__ & __getitem__\n",
        "        __getitem__ should return \n",
        "            data:\n",
        "                input_ids\n",
        "                attention_masks\n",
        "                text\n",
        "                targets\n",
        "        \"\"\"\n",
        "        row = self.df.iloc[index]\n",
        "        text1, text2, label = self.get_input_data(row)\n",
        "\n",
        "        # Encode_plus will:\n",
        "        # (1) split text into token\n",
        "        # (2) Add the '[CLS]' and '[SEP]' token to the start and end\n",
        "        # (3) Truncate/Pad sentence to max length\n",
        "        # (4) Map token to their IDS\n",
        "        # (5) Create attention mask\n",
        "        # (6) Return a dictionary of outputs\n",
        "        encoding = self.tokenizer(\n",
        "            text1,\n",
        "            text2,\n",
        "            truncation=True,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'text1': text1,\n",
        "            'text2': text2,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_masks': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "    def labelencoder(self,text):\n",
        "        if text==True:\n",
        "            return 1\n",
        "        elif text==False:\n",
        "            return 0\n",
        "        else:\n",
        "            return 2\n",
        "\n",
        "    def get_input_data(self, row):\n",
        "        # Preprocessing: {remove icon, special character, lower}\n",
        "        text1 = row['question 1']\n",
        "        text2 = row['question 2']\n",
        "        label = self.labelencoder(row['similarity'])\n",
        "\n",
        "        return text1, text2, label"
      ],
      "metadata": {
        "id": "OY_CZ8mDRTfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_datasets = EquavilenceDataset(train_df, tokenizer, max_len=128)\n",
        "tokenizer_validations = EquavilenceDataset(valid_df, tokenizer, max_len=128)"
      ],
      "metadata": {
        "id": "J99-WjwlS0ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_datasets[1]"
      ],
      "metadata": {
        "id": "DXQy-uIbTFJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7edbc2-5ae8-4cf3-f627-9998a035f4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text1': 'người việt_nam bay vào vũ_trụ đầu_tiên tên là gì',\n",
              " 'text2': 'ai là người vn đầu_tiên bay vào vũ_trụ',\n",
              " 'input_ids': tensor([    0,    18, 56444,   542,   620,    33,  4196,   191,   221,     8,\n",
              "           148,     2,     2,   277,     8,    18, 33756,   191,   620,    33,\n",
              "          4196,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
              " 'attention_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'targets': tensor(1)}"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_validations[3]"
      ],
      "metadata": {
        "id": "cNCD9yehULw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462b0fc9-1475-441f-8194-fbdaa3df819f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text1': 'album đầu_tay của adele tên gì',\n",
              " 'text2': 'album đầu_tay của adele tên gì',\n",
              " 'input_ids': tensor([    0,  2675,  7467,     7,  2811, 15156,   221,   148,     2,     2,\n",
              "          2675,  7467,     7,  2811, 15156,   221,   148,     2,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
              "             1,     1,     1,     1,     1,     1,     1,     1]),\n",
              " 'attention_masks': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'targets': tensor(1)}"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "train_loader = DataLoader(tokenizer_datasets, batch_size=16, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(tokenizer_validations, batch_size=16, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "Hcso3mVkKYOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EquivalenceClassifier(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(EquivalenceClassifier, self).__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "        nn.init.normal_(self.fc.weight, std=0.02)\n",
        "        nn.init.normal_(self.fc.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        last_hidden_state, output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=False # Dropout will errors if without this\n",
        "        )\n",
        "\n",
        "        x = self.drop(output)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "85rfcKcm__Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, train_loader):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    for data in tqdm(train_loader):\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_masks'].to(device)\n",
        "        targets = data['targets'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "        correct += torch.sum(pred == targets)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "    print(f'Train Accuracy: {correct.double()/len(train_loader.dataset)} Loss: {np.mean(losses)}')\n",
        "\n",
        "def eval(test_data = False):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        data_loader = test_loader if test_data else valid_loader\n",
        "        for data in data_loader:\n",
        "            input_ids = data['input_ids'].to(device)\n",
        "            attention_mask = data['attention_masks'].to(device)\n",
        "            targets = data['targets'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "\n",
        "            _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "            correct += torch.sum(pred == targets)\n",
        "            losses.append(loss.item())\n",
        "    \n",
        "    if test_data:\n",
        "        print(f'Test Accuracy: {correct.double()/len(test_loader.dataset)} Loss: {np.mean(losses)}')\n",
        "        return correct.double()/len(test_loader.dataset)\n",
        "    else:\n",
        "        print(f'Valid Accuracy: {correct.double()/len(valid_loader.dataset)} Loss: {np.mean(losses)}')\n",
        "        return correct.double()/len(valid_loader.dataset)"
      ],
      "metadata": {
        "id": "DrE_hO2vVD4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    fold = 0\n",
        "    \n",
        "    model = EquivalenceClassifier(n_classes=2).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # Recommendation by BERT: lr: 5e-5, 2e-5, 3e-5\n",
        "    # Batchsize: 16, 32\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "    \n",
        "    lr_scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer, \n",
        "                num_warmup_steps=0, \n",
        "                num_training_steps=len(train_loader)*EPOCHS\n",
        "            )\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'Epoch {epoch+1}/{EPOCHS}')\n",
        "        print('-'*30)\n",
        "\n",
        "        train(model, criterion, optimizer, train_loader)\n",
        "        val_acc = eval()\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            torch.save(model.state_dict(), f'phobert_fold{fold+1}.pth')\n",
        "            best_acc = val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5HUfY7fYq1T",
        "outputId": "4da5a8d1-fc5f-4204-864a-207b796e9115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/6\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5448/5448 [32:35<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9385820989708228 Loss: 0.1814297745914806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.9783696529459242 Loss: 0.07635492277819844\n",
            "Epoch 2/6\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5448/5448 [32:35<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9747008272427918 Loss: 0.09233951497921487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.9816787732041969 Loss: 0.06973826334530098\n",
            "Epoch 3/6\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5448/5448 [32:33<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9847746021547323 Loss: 0.060999664868947805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.9855528652138822 Loss: 0.06841840383622066\n",
            "Epoch 4/6\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5448/5448 [32:34<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.990626111499937 Loss: 0.04001598312717117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.9861985472154964 Loss: 0.07158312859387332\n",
            "Epoch 5/6\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5448/5448 [32:34<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9935403926248035 Loss: 0.02824391678059101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.9858757062146893 Loss: 0.07002394887790526\n",
            "Epoch 6/6\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5448/5448 [32:34<00:00,  2.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9961907821517492 Loss: 0.01708189489478365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid Accuracy: 0.9870056497175141 Loss: 0.07039585694381986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def test(data_loader):\n",
        "    model = EquivalenceClassifier(n_classes=2)\n",
        "    model.to(device)\n",
        "    model.load_state_dict(torch.load(f'phobert_fold{fold+1}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    texts1 = []\n",
        "    texts2 = []\n",
        "    predicts = []\n",
        "    predict_probs = []\n",
        "    real_values = []\n",
        "\n",
        "    for data in tqdm(data_loader):\n",
        "        text1 = data['text1']\n",
        "        text2 = data['text2']\n",
        "        input_ids = data['input_ids'].to(device)\n",
        "        attention_mask = data['attention_masks'].to(device)\n",
        "        targets = data['targets'].to(device)\n",
        "\n",
        "        total_outs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "        total_outs.append(outputs)\n",
        "        \n",
        "        total_outs = torch.stack(total_outs)\n",
        "        _, pred = torch.max(total_outs.mean(0), dim=1)\n",
        "        texts1.extend(text1)\n",
        "        texts2.extend(text2)\n",
        "        predicts.extend(pred)\n",
        "        predict_probs.extend(total_outs.mean(0))\n",
        "        real_values.extend(targets)\n",
        "    \n",
        "    predicts = torch.stack(predicts).cpu()\n",
        "    predict_probs = torch.stack(predict_probs).cpu()\n",
        "    real_values = torch.stack(real_values).cpu()\n",
        "    print(classification_report(real_values, predicts))\n",
        "    return real_values, predicts"
      ],
      "metadata": {
        "id": "5jjQI6odK6YS"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = EquavilenceDataset(test_df, tokenizer, max_len=50)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "real_values, predicts = test(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47buenrtLZso",
        "outputId": "5d035fd6-31e7-4711-fb8c-78e7cacb0774"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            " 53%|█████▎    | 1236/2335 [00:57<00:49, 22.17it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            " 53%|█████▎    | 1239/2335 [00:57<00:48, 22.37it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "100%|██████████| 2335/2335 [01:46<00:00, 21.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99     29215\n",
            "           1       0.96      0.97      0.96      8139\n",
            "\n",
            "    accuracy                           0.98     37354\n",
            "   macro avg       0.97      0.98      0.98     37354\n",
            "weighted avg       0.98      0.98      0.98     37354\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['False', 'True']\n",
        "\n",
        "def infer(text1, text2, tokenizer, max_len=128):\n",
        "    text1 = word_segment(text1)\n",
        "    text2 = word_segment(text2)\n",
        "    encoded_review = tokenizer.encode_plus(\n",
        "        text1,\n",
        "        text2,\n",
        "        max_length=max_len,\n",
        "        truncation=True,\n",
        "        add_special_tokens=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    input_ids = encoded_review['input_ids'].to(device)\n",
        "    attention_mask = encoded_review['attention_mask'].to(device)\n",
        "\n",
        "    output = model(input_ids, attention_mask)\n",
        "    _, y_pred = torch.max(output, dim=1)\n",
        "\n",
        "    print(f'Text1: {text1}')\n",
        "    print(f'Text2: {text2}')\n",
        "    print(f'Sentiment: {class_names[y_pred]}')"
      ],
      "metadata": {
        "id": "_Y0fDrtuNnEW"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"tên thật của bác giáp là gì\"\n",
        "text2 = \"tên thật của cụ võ nguyên giáp\""
      ],
      "metadata": {
        "id": "gQN-Nl59PgfQ"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer(text1, text2, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhzpNAXtOCR6",
        "outputId": "dfc57e0f-b39c-43c0-e2f8-c173ab5e6dc3"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text1: tên thật của bác giáp là gì\n",
            "Text2: tên thật của cụ võ nguyên giáp\n",
            "Sentiment: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmvOwEwVT-ie"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}